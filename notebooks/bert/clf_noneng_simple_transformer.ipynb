{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a6a889",
   "metadata": {},
   "source": [
    "# BERT\n",
    "---\n",
    "## Giriş\n",
    "\n",
    "BERT ilk olarak, Jacob Devlin, Ming-Wei Chang, Kenton Lee ve Kristina Toutanova tarafından [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) adlı makalede sunulmuştur. Bidirectional bir transformer modelidir, Toronto Book Corpus ve Wikipedia'dan oluşan büyük bir corpus ile eğitilmiştir. Eğitimi için maskelenmiş dil modeli objective ve next sentence prediction kombinasyonu kullanılmıştır. \n",
    "\n",
    "Repo: [google-research/bert](https://github.com/google-research/bert) \n",
    "\n",
    "## Paper: Abstract\n",
    "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "\n",
    "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n",
    "\n",
    "## İpuçları\n",
    "\n",
    "- BERT bünyesinde mutlak __pozisyon embedding__'leri barındırır. Bu yüzden \"padding\" işlemi cümlenin sağ tarafına doğru yapılmalıdır.\n",
    "- BERT __masked language modeling__ (__MLM__) ve __next sentence prediction__ (__NSP__) görevleriyle eğitilmiştir. Maskelenmiş tokenleri tahmin etmekte ve __natural language understanding__'de (__NLU__) başarılıdır. Fakat metin üretmek için optimal değildir.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c12d5",
   "metadata": {},
   "source": [
    "## BERT Text Classification (Non-English)\n",
    "\n",
    "Dünya üzerinde 7.5 milyar insan ve 200'den fazla ulus yaşamaktadır ancak bunlardan sadece 1.2 milyarının ana dili İngilizce'dir. Bu nedenle, dünya üzerinde büyük miktarlarda İngilizce olmayan metin verisi olduğu söylenilebilir. \n",
    "\n",
    "Çoğu BERT tutorial'ı İngilizce dili üzerinden ilerler ve çeşitli problemleri İngilizce için nasıl çözüldüğünü anlatır. Bu yazıda farklı diller için eğitim adımlarından bahsedilecektir. \n",
    "\n",
    "Deep learning alanında şu sıralar populer bir tartışma var: Dil modeli çok dilli olmalı vs tek dilli olmalı. Aslında ikisi de yapılabilir. Çok dilli modeller, anlaşılacağı gibi, birden fazla dili anlama becerisine sahiptir. Örnek olarak Google Research tarafından sunulan mBERT verilebilir. Öte yandan tek dilli modeller, sadece bir dili anlayabilirler.\n",
    "\n",
    "Çok dilli modeller, birçok modelde başarı gösterirler ancak boyut olarak büyüklerdir, eğitimleri için daha çok zaman ve veri gereklidir. Bu yüksek eğitim maliyeti anlamına gelir. Bu yüzden, bu yazıda tek dilli, İngilizce olmayan, BERT tabanlı ve çok etiketli bir sınıflandırıcı eğitimi örneği ele alınacaktır. \n",
    "\n",
    "### Tutorial\n",
    "Bu yazıda Simple Transformer kütüphanesi kullanılacaktır. Bu aslında bir NLP kütüphanesidir ve HuggingFace Transformer kütüphanesi üzerine kurulmuştur. Transformer modellerini birkaç satırda fine-tune etmemize imkan tanır. Kullanılacak veri seti, Germeval 2019'dir: Almanca tweet'lerden oluşur. Bu veri seti ile saldırgan dilde yazılmış tweet'ler ayırt edilmeye çalışılacaktır. Tweet'ler 4 kategoriye bölünmüştür: `PROFANITY`, `INSULT`, `ABUSE` ve `OTHERS`. Bu veri seti üzerinde erişilen en yüksek skor. `0.7361`'dir. Adımlar aşağıdaki gibi özetlenmiştir:\n",
    "\n",
    "- _Simple Transformer_ kütüphanesi kurulumu\n",
    "- Pre-trained monolingual model seçimi\n",
    "- Veri seti yükleme\n",
    "- Fine-tune gerçekleştirme\n",
    "- Sonuçların değerlendirilmesi\n",
    "- Eğitilen modelin kaydedilmesi\n",
    "- Modelin gerçek örnek ile denenmesi\n",
    "\n",
    "### _Simple Transformers_ kurulumu\n",
    "Kütüphane, `pip` üzerinden yüklenebilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19e1e320",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (0.63.9)\n",
      "Requirement already satisfied: numpy in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (1.24.2)\n",
      "Requirement already satisfied: transformers>=4.6.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (4.26.1)\n",
      "Requirement already satisfied: streamlit in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (1.19.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (1.2.1)\n",
      "Requirement already satisfied: tokenizers in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (0.13.2)\n",
      "Requirement already satisfied: requests in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (2.28.2)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (0.13.10)\n",
      "Requirement already satisfied: seqeval in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (1.10.1)\n",
      "Requirement already satisfied: pandas in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (1.5.3)\n",
      "Requirement already satisfied: regex in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (4.64.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (0.1.97)\n",
      "Requirement already satisfied: datasets in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (2.10.0)\n",
      "Requirement already satisfied: tensorboard in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from simpletransformers) (2.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from transformers>=4.6.0->simpletransformers) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from transformers>=4.6.0->simpletransformers) (23.0)\n",
      "Requirement already satisfied: filelock in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from transformers>=4.6.0->simpletransformers) (3.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (65.6.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
      "Requirement already satisfied: setproctitle in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (3.1.31)\n",
      "Requirement already satisfied: pathtools in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (5.9.4)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from wandb>=0.10.32->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from requests->simpletransformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from requests->simpletransformers) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from requests->simpletransformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from requests->simpletransformers) (2022.12.7)\n",
      "Requirement already satisfied: responses<0.19 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from datasets->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from datasets->simpletransformers) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from datasets->simpletransformers) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from datasets->simpletransformers) (2023.1.0)\n",
      "Requirement already satisfied: multiprocess in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from datasets->simpletransformers) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from datasets->simpletransformers) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from datasets->simpletransformers) (3.8.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from pandas->simpletransformers) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.2.2)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.2)\n",
      "Requirement already satisfied: pympler>=0.9 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (1.0.1)\n",
      "Requirement already satisfied: tzlocal>=1.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.2)\n",
      "Requirement already satisfied: toml in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (5.3.0)\n",
      "Requirement already satisfied: blinker>=1.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (1.5)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (13.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (9.4.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.8.0)\n",
      "Requirement already satisfied: validators>=0.2 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (0.20.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (6.0.0)\n",
      "Requirement already satisfied: semver in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (2.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from streamlit->simpletransformers) (4.5.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (1.51.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tensorboard->simpletransformers) (0.38.4)\n",
      "Requirement already satisfied: entrypoints in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
      "Requirement already satisfied: toolz in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.17.3)\n",
      "Requirement already satisfied: jinja2 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.4.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.15.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.14.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.14.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.2.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from validators>=0.2->streamlit->simpletransformers) (5.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.19.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=10.11.0->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.2)\n",
      "Requirement already satisfied: tzdata in /Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit->simpletransformers) (2022.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a989f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Pre-trained monolingual model seçimi\n",
    "Kurulumu yapılan kütüphane, HuggingFace Transformer kütüphanesi üzerine kuruludur. Bu yüzden HuggingFace Hub'daki Transformer Library içinde tanımlanmış bütün pre-trained modeller için çalışacaktır. Topluluk tarafından yüklenen modellerin listesine [link](https://huggingface.co/models) üzerinden ulaşılabilir.\n",
    "\n",
    "Örnekte `distilbert-base-german-cased` modeli kullanılacaktır. Bu model BERT'in daha küçük, daha hızlı ve daha maliyetsiz versiyonudur. BERT'e göre parametre sayısı %40 daha azdır ve %60 daha hızlı çalışır. \n",
    "\n",
    "---\n",
    "\n",
    "### Veri seti yükleme\n",
    "Veri seti iki adet metin belgesinde saklanmıştır ve https://fz.h-da.de/iggsa/data adresinden indirilebilir. İndirilen veriler `pandas` ile bir DataFrame'e dönüştürülebilir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "911d2429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15418, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@JanZimmHHB @mopo Komisch das die RealitÃ¤tsve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@faznet @Gruene_Europa @SPDEuropa @CDU CDU ste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DLFNachrichten Die Gesichter, Namen, Religion...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@welt Wie verwirrt muss man sein um sich zu we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@hacker_1991 @torben_braga Weil die AfD den Fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  pred_class\n",
       "0  @JanZimmHHB @mopo Komisch das die RealitÃ¤tsve...           0\n",
       "1  @faznet @Gruene_Europa @SPDEuropa @CDU CDU ste...           1\n",
       "2  @DLFNachrichten Die Gesichter, Namen, Religion...           3\n",
       "3  @welt Wie verwirrt muss man sein um sich zu we...           1\n",
       "4  @hacker_1991 @torben_braga Weil die AfD den Fe...           1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_list = [\"INSULT\", \"ABUSE\", \"PROFANITY\", \"OTHER\"]\n",
    "df1 = pd.read_csv(\"../data/raw/germeval2019GoldLabelsSubtask1_2.txt\",\n",
    "                  sep='\\t',\n",
    "                  lineterminator=\"\\n\",\n",
    "                  encoding=\"utf-8\",\n",
    "                  names=[\"tweet\", \"task1\", \"task2\"])\n",
    "df2 = pd.read_csv(\"../data/raw/germeval2019.training_subtask1_2_korrigiert.txt\",\n",
    "                  sep='\\t',\n",
    "                  lineterminator=\"\\n\",\n",
    "                  encoding=\"utf-8\",\n",
    "                  names=[\"tweet\", \"task1\", \"task2\"])\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "df['task2'] = df['task2'].str.replace('\\r', '')\n",
    "df['pred_class'] = df.apply(\n",
    "    lambda x: class_list.index(x['task2']),\n",
    "    axis=1)\n",
    "\n",
    "df = df[['tweet', 'pred_class']]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a042a84",
   "metadata": {},
   "source": [
    "Bir test veri seti olmadığı için, veri setinin %10'u test için ayrılabilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4ab4729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (13876, 2)\n",
      "test shape:  (1542, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1)\n",
    "\n",
    "print('train shape: ', train_df.shape)\n",
    "print('test shape: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b6353",
   "metadata": {},
   "source": [
    "---\n",
    "### Pre-trained modelin yüklenmesi\n",
    "Bu adımda, pre-trained modelin çalışmaya dahil edilmesi gösterilecektir. `ClassificationModel` sınıfına ait bir instance oluşturarak gerçekleştirilebilir. Bu instance şu parametreleri alır:,\n",
    "- __mimari__: bu örnek için `\"bert\"`\n",
    "- __pre-trained model__: `\"distilbert-base-german-cased\"`\n",
    "- __sınıf sayısı__: `4`\n",
    "- __hiperparametreler__: `train_args`\n",
    "\n",
    "Hiperparametreler oldukça farklı şekilde inşa edilebilir. Detaylar için _Simple Transformer_ dökümantasyonuna bakılabilir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0abdeb9d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6358cffd9cfd4b8a918ddbbe028a478c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/270M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.sa_layer_norm.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'classifier.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'classifier.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'pooler.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06d06815eb5459e8a2564d5a8cc6173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85d6c4bb67241afac9c9aebdb8f4622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7665ff40fc8d4e27a30876bf38e4951d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/479k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "# Hiperparametre tanımları\n",
    "train_args = {\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"fp16\": False,\n",
    "    \"num_train_epochs\": 4,\n",
    "}\n",
    "\n",
    "# ClassificationModel instance'ı oluşturmak\n",
    "model = ClassificationModel(\n",
    "    \"bert\",\n",
    "    \"distilbert-base-german-cased\",\n",
    "    num_labels=4,\n",
    "    use_cuda=False,\n",
    "    args=train_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4258bb",
   "metadata": {},
   "source": [
    "--- \n",
    "### Modelin fine-tune edilmesi\n",
    "Modeli fine-tune edebilmek için `model.train_model()` metodunun çağrılması gereklidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alihantadal/Documents/GitHub/transformers/venv/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730314103cb440d39c04e4d0f6bd5c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632d466a27194f54af5b2a6098a3fbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46dbecd25ee4f6382e82f43a96e194f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/1735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73350215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
