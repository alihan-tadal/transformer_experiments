{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f2094e",
   "metadata": {},
   "source": [
    "# Giriş\n",
    "\n",
    "Önceki kısımda tokenizasyon işleminin tek bir cümle için ne kadar kolay olduğunu gördük. Ancak konu birden fazla cümleyle çalışmaya gelince bazı soru işaretleri ortaya çıkabilir.\n",
    "- Multiple sequence veri ile nasıl başa çıkılabilir?\n",
    "- Farklı uzunluktaki multiple sequencei le nasıl baş edilebilir?\n",
    "- Vocab. indisleri, modelin iyi çalışmasındaki tek parametre midir?\n",
    "- Çok uzun sequence'lar bir problem yaratır mı?\n",
    "\n",
    "## Girdi olarak batch kabul eden modeller\n",
    "\n",
    "Önceki bölümlerde, bir ID listesini nasıl oluşturabileceğimizi gördük. Şimdi de bu listeleri tensor'e çevirerek modele vermeyi deneyebiliriz.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)\n",
    "```\n",
    "⚙️ Çıktı:\n",
    "\n",
    "```python\n",
    "IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
    "```\n",
    "\n",
    "Örnek kod çalıştırıldığında yukarıdaki gibi bir hata ile karşılaşılır. Bunun sebebi HuggingFace Transformer modelleri default olarak multiple sequence input kabul eder. Yukarıdaki işlemleri tokenizer'in her aşamasını manuel gerçekleştirerek ilerlettik. Normal durumda, `tokenizer()` direkt olarak çağrıldığında, aşağıdaki id'ler oluşturulur.\n",
    "```python\n",
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "tokenized_inputs[\"input_ids\"]\n",
    "```\n",
    "⚙️ Çıktı:\n",
    "```python\n",
    "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
    "          2607,  2026,  2878,  2166,  1012,   102]])\n",
    "```\n",
    "\n",
    "Ancak modele verdiğimiz tensor aşağıdaki gibidir:\n",
    "```python\n",
    "tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
    "         2026,  2878,  2166,  1012])\n",
    "```\n",
    "\n",
    "Bu boyut farkından dolayı, kod hata vermiştir. Düzeltmek için aşağıdaki değişimi yapabiliriz:\n",
    "\n",
    "```python\n",
    "#input_ids = torch.tensor(ids)\n",
    "input_ids = torch.tensor([ids])\n",
    "```\n",
    "\n",
    "_Batching_, modele birden fazla cümle gönderme işlemidir ve bir adet cümle ile çalışmak kadar basittir. Fakat ikinci bir problem sözkonusudur. İki veya daha fazla cümleyi batch ettiğimizde bu cümlelerin uzunlukları birbirinden farklı olabilir. Buradaki problem tensor'lerin her durumda dikdörtgen olması gerekliliğidir. Bu yüzden farklı uzunluktaki (token uzunluğu) cümleleri direkt olarak tensor'lere çevirmek mümkün değildir. Buna çözüm olarak _padding_ uygulanabilir.\n",
    "\n",
    "## Padding the inputs\n",
    "Yukarıda bahsedilen nedenden dolayı aşağıdaki liste bir tensor'e çevrilemez:\n",
    "```python\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "```\n",
    "Padding uygulamak için bir `padding_token` tanımlanması gerekir. Cümle uzunluğu en uzun cümleye göre belirlendikten sonra daha kısa olan cümlelere padding uygulanabilir.\n",
    "\n",
    "Tokenizer'a ait padding token id'ye ulaşabilmek için `tokenizer.pad_token_id` özelliğine bakılabilir. Aşağıda birbirinden farklı uzunlukta olan cümlelerin padding işlemi gerçekleştirilmiştir.\n",
    "\n",
    "```python\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)\n",
    "```\n",
    "⚙️Çıktı:\n",
    "```python\n",
    "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\n",
    "```\n",
    "Burada bir terslik vardır. Birinci ve ikinci cümlelerin logit'leri tekil ve batch olarak verildiğinde farklı logit değerleri üretilmiştir. Bunun sebebi, Transformer modellerinin verilen sequence'ı bir bağlamda incelemesidir. Eklediğimiz padding token'leri bu bağlamdan ayırmak için __attention_mask__ tanımlanması gereklidir.\n",
    "\n",
    "## Attention masks\n",
    "\n",
    "Attention mask'lar input id'lerden oluşan tensor ile aynı boyuttadır ve 0 veya 1 değerlerinden oluşur. Model tarafından görmezden gelinmesi istenilen token'lara karşılık gelen attention_mask tensor elementi, 0'a eşitlenir.\n",
    "\n",
    "```python\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)\n",
    "```\n",
    "⚙️Çıktı: \n",
    "```python\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
    "```\n",
    "\n",
    "Bu sayede batch sequence çıktısı ile tekil sequence çıktısı aynı olur.\n",
    "\n",
    "## Longer sequences\n",
    "\n",
    "Transformer modellerinde, input olarak verilen sequence için bir uzunluk kısıtı vardır. Çoğu model 512-1024 token uzunluğu ile başa çıkabilir, aksi takdirde de çalışmayı durdurur. İki çözümü olabilir:\n",
    "\n",
    "1. Sequence'ları kısaltmak.\n",
    "2. Daha uzun sequence'ları kabul edebilen modeller ile çalışmak.\n",
    "\n",
    "Modellerin başarılı olabileceği sequence uzunluğu farklıdır ve bazı modeller uzun sequence'lar ile çalışmak için geliştirilmiştir. Örnek olarak __Longformer__ veya __LED__ verilebilir. Eğer iş daha uzun sequence'lar ile çalışmayı gerektiriyorsa bu iki model önerilebilir.\n",
    "\n",
    "Aksi durumda, sequence'ları kırpmak bir seçenektir, aşağıdaki gibi gerçekleştirilebilir:\n",
    "```python\n",
    "sequence = sequence[:max_sequence_length]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d40516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
