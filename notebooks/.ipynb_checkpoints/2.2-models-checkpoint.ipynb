{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5413719c",
   "metadata": {},
   "source": [
    "# Giriş\n",
    "\n",
    "Bu bölümde bir modeli kullanmak ve bir model yaratmak detaylı olarak incelenecektir. Herhangi bir checkpoint'ten bir model başlatmak için `AutoModel` sınıfı kullanılacaktır.\n",
    "\n",
    "AutoModel ve benzer sınıflar özünde kütüphanedeki modeller için birer wrapper'dır. Bu wrapper'lar, verilen checkpoint'e göre modelin mimarisi için bir çıkarımda bulunabilirler ve bu mimaride bir model oluştururlar.\n",
    "\n",
    "Fakat kullanılacak model tipi biliniyorsa mimariyi tanımlayan sınıf direkt olarak kullanılabilir. Yazının devamında, bütün bunların BERT için nasıl işlediği anlatılacaktır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b512e",
   "metadata": {},
   "source": [
    "## Transformer Oluşturmak\n",
    "\n",
    "Bir BERT modeli oluşturmak için yapılması gereken ilk şey bir `config` objesi oluşturmaktır. Bu obje, modeli build ederken kullanılacak birçok özellik içerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4a0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b15380",
   "metadata": {},
   "source": [
    "Yukarıdaki bütün özellikler henüz anlatılmadı fakat bazılarını hatırlamakta fayda var. `hidden_size`, hidden state vektörünün boyutunu tanımlar. `num_hidden_layers` parametresi, transformer modelinin sahip olduğu layer sayısını tanımlar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87c8da",
   "metadata": {},
   "source": [
    "## Different loading methods\n",
    "\n",
    "Default configuration ile model üretmek, modeli random değerler ile başlatır. Model bu şekilde kullanılabilir ama çıktısı hiçbir anlama gelmeyecektirir, bu yüzden eğitilmesi lazımdır. Ancak daha önce tartışıldığı gibi, bu fazlaca efor demektir. Bu yüzden önceden eğitilmiş modellerden faydalanmak gerekir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada92b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbd48d",
   "metadata": {},
   "source": [
    "Daha önceden eğitilmiş bir modeli yüklemek oldukça kolaydır:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df03b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912b988",
   "metadata": {},
   "source": [
    "Daha önceden gördüğümüz gibi, BertModel ve AutoModel ile yer değiştirebilir. Checkpoint agnostik kod üretebilmek için bundan sonra AutoModel ile devam edeceğiz. Bu sayede eğer kod bir checkpoint için çalışıyorsa, diğer checkpointler için de çalışacaktır. Dahası, farklı mimariler için bile çalışmaya devam edecektir (task aynı olduğu sürece). \n",
    "\n",
    "Yukarıdaki modelde, BertConfig sınıfını kullanmadık. Bunun yerine pretrained modeli yükledik. Bu model checkpoint'i BERT author'ları tarafından oluşturulmuştur ve detayları model card sayfasında bulunabilir.\n",
    "\n",
    "Yukarıdaki modelde, checkpoint'in ağırlıklarını yüklemiştir bu yüzden kullanıma hazırdır; bir train işlemine gerek yoktur. Fakat istenirse fine-tuning de mümkündür. \n",
    "\n",
    "Model ağırlıkları indirilip `~/.cache/huggingface/transformers` yolunda saklanır. Bu yüzden modeli sonradan yüklemek istediğinizde süreç daha hızlı sonlanacaktır. Bu yolu değiştirmek kolaydır, bunun için dökümantasyonu inceleyebilirsiniz.\n",
    "\n",
    "## Saving models\n",
    "\n",
    "Model kaydetmek, yüklemek kadar basittir ve `model.save_pretrained()` metodu ile gerçekleştirilebilir. Bu işlem sonucunda iki adet dosya diskinize kaydedilecektir:\n",
    "- config.json\n",
    "- pytorch_model.bin\n",
    "\n",
    "`config.json`, mimariyi inşa etmek için gerekli bilgileri ve metadata içerir.\n",
    "`pytorch_model.bin`, modelin ağırlıklarını saklar, `state_dictionary` olarak da bilinir.\n",
    "\n",
    "## Using a Transformer model for inference\n",
    "\n",
    "Bu aşamada modeli kullanmaya başlayabiliriz. Fakat transformer modelleri sadece nümerik girdiler ile çalışabilir (tokenizer tarafından üretilen nümerik girdiler). Tokenizer'lar hakkında calısmaya başlamadan önce modelin kabul ettiği girdileri inceleyelim.\n",
    "\n",
    "Tokenizer'lar, modelin üzerinde çalıştığı backend'e göre inputları farklı tiplere otomatik olarak cast edebilir. Örneğin elimizde aşağıdaki sequence'lar olsun:\n",
    "``` python \n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "```\n",
    "\n",
    "Encode edilmiş çıktı şuna benzeyecektir:\n",
    "``` python\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "```\n",
    "Bu listelerden oluşan bir listedir. Tensör'ler sadece dikdörtgen şekilde olabilir, matrisler gibi. Elimizdeki encoded_sequence uygun formda olduğundan bunu bir tensor'e dönüştürmek kolaydır:\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "```\n",
    "Üretilen bu tensor'leri modeller ile kullanmak oldukça kolaydır. Aşağıdaki kod modelin çıktılarını üretmek için kullanılabilir:\n",
    "``` python\n",
    "output = model(model_inputs)\n",
    "```\n",
    "\n",
    "Model birçok argüman kabul edebilir ancak sadece ID'ler gereklidir. Alabileceği argümanlar ve görevleri ileride tartışılacaktır. İlerlemeden önce tokenizer'lerin detaylı bir şekilde incelenmesinde fayda vardır. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb8edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e391a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
